varImpPlot(rf_model)
# Display mean decrease in Gini
print(importance(rf_model))
# Shapley values using the iml package
predictor <- Predictor$new(rf_model, data = data[predictors], y = as.numeric(data[[response]]))
shapley <- Shapley$new(predictor, x.interest = data[1, predictors])
shapley_values <- as.data.frame(shapley$results)
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
shapley_values_vector <- setNames(shapley_values_summary$phi, shapley_values_summary$feature)
# Ensure all components have matching lengths
print(length(predictors))                    # Should be the number of predictors
print(length(correlations))                  # Should match the number of predictors
print(length(standardized_coefs))            # Should match the number of predictors
print(length(usefulness))                    # Should match the number of predictors
print(length(mean_decrease_gini))            # Should match the number of predictors
print(length(shapley_values_vector))         # Should match the number of predictors
# Combine all results into a single data frame
results <- data.frame(
Variable = predictors,
Pearson_Correlation = correlations,
Standardized_Coefficients = standardized_coefs,
Usefulness = usefulness,
Mean_Decrease_Gini = mean_decrease_gini,
Shapley_Values = shapley_values_vector[predictors]
)
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Assess model performance
print(rf_model)
# Plot model performance
plot(rf_model)
# Plot mean decrease in Gini
varImpPlot(rf_model)
# Install necessary packages if not already installed
required_packages <- c("tidyverse", "car", "randomForest", "iml", "data.table", "MASS", "relaimpo", "caret", "e1071", "xgboost")
new_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load necessary libraries
library(tidyverse)
library(car)
library(randomForest)
library(iml)
library(data.table)
library(MASS)
library(relaimpo)
library(caret)
library(knitr)
library(e1071)
library(xgboost)
# Load the data
data <- read.csv("/Users/danielgarciagomar/Documents/UCSD/MPP/Spring/Marketing Analytics/Quarto/quarto_website/projects/project4/data_for_drivers_analysis.csv")
data$satisfaction <- as.factor(data$satisfaction)  # Convert the response variable to a factor
# Verify the levels of the factor
print(levels(data$satisfaction))
# Display structure and first few rows to understand the data
str(data)
head(data)
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Plot model performance
plot(rf_model)
# Plot mean decrease in Gini
varImpPlot(rf_model)
# Display mean decrease in Gini
print(importance(rf_model))
# Shapley values using the iml package
predictor <- Predictor$new(rf_model, data = data[predictors], y = as.numeric(data[[response]]))
shapley <- Shapley$new(predictor, x.interest = data[1, predictors])
shapley_values <- as.data.frame(shapley$results)
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
shapley_values_vector <- setNames(shapley_values_summary$phi, shapley_values_summary$feature)
# Ensure all components have matching lengths
print(length(predictors))                    # Should be the number of predictors
print(length(correlations))                  # Should match the number of predictors
print(length(standardized_coefs))            # Should match the number of predictors
print(length(usefulness))                    # Should match the number of predictors
print(length(mean_decrease_gini))            # Should match the number of predictors
print(length(shapley_values_vector))         # Should match the number of predictors
# Combine all results into a single data frame
results <- data.frame(
Variable = predictors,
Usefulness = usefulness,
Mean_Decrease_Gini = mean_decrease_gini,
Shapley_Values = shapley_values_vector[predictors],
Standardized_Coefficients = standardized_coefs,
Pearson_Correlation = correlations
)
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Plot model performance
plot(rf_model)
# Plot mean decrease in Gini
varImpPlot(rf_model)
# Display mean decrease in Gini
print(importance(rf_model))
# Shapley values using the iml package
predictor <- Predictor$new(rf_model, data = data[predictors], y = as.numeric(data[[response]]))
shapley <- Shapley$new(predictor, x.interest = data[1, predictors])
shapley_values <- as.data.frame(shapley$results)
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
shapley_values_vector <- setNames(shapley_values_summary$phi, shapley_values_summary$feature)
# Ensure all components have matching lengths
print(length(predictors))                    # Should be the number of predictors
print(length(correlations))                  # Should match the number of predictors
print(length(standardized_coefs))            # Should match the number of predictors
print(length(usefulness))                    # Should match the number of predictors
print(length(mean_decrease_gini))            # Should match the number of predictors
print(length(shapley_values_vector))         # Should match the number of predictors
# Combine all results into a single data frame
#results <- data.frame(
#  Variable = predictors,
#  Usefulness = usefulness,
#  Mean_Decrease_Gini = mean_decrease_gini,
#  Shapley_Values = shapley_values_vector[predictors],
#  Standardized_Coefficients = standardized_coefs,
#  Pearson_Correlation = correlations
#)
# Identify common predictors across all metrics
common_predictors <- intersect(intersect(intersect(intersect(
names(correlations), names(standardized_coefs)), names(usefulness)),
names(mean_decrease_gini)), names(shapley_values_vector))
# Subset all metrics to common predictors
correlations <- correlations[common_predictors]
standardized_coefs <- standardized_coefs[common_predictors]
usefulness <- usefulness[common_predictors]
mean_decrease_gini <- mean_decrease_gini[common_predictors]
shapley_values_vector <- shapley_values_vector[common_predictors]
# Combine all results into a single data frame
results <- data.frame(
Variable = common_predictors,
Usefulness = usefulness,
Mean_Decrease_Gini = mean_decrease_gini,
Shapley_Values = shapley_values_vector,
Standardized_Coefficients = standardized_coefs,
Pearson_Correlation = correlations
)
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Train an XGBoost model
dtrain <- xgb.DMatrix(data = as.matrix(data[predictors]), label = as.numeric(data[[response]]))
xgb_model <- xgboost(data = dtrain, max.depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror", verbose = 0)
# Extract feature importance from XGBoost model
xgb_importance <- xgb.importance(model = xgb_model)
xgb_importance_scores <- xgb_importance$Gain
names(xgb_importance_scores) <- xgb_importance$Feature
# Shapley values using the iml package with improved setup
# Ensure the predictor and response data are correctly passed to the iml Predictor
predictor <- Predictor$new(rf_model, data = standardized_data[predictors], y = as.numeric(data[[response]]))
# Calculate Shapley values for each predictor
shapley <- Shapley$new(predictor)
shapley_values <- shapley$results
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Train an XGBoost model
dtrain <- xgb.DMatrix(data = as.matrix(data[predictors]), label = as.numeric(data[[response]]))
xgb_model <- xgboost(data = dtrain, max.depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror", verbose = 0)
# Extract feature importance from XGBoost model
xgb_importance <- xgb.importance(model = xgb_model)
xgb_importance_scores <- xgb_importance$Gain
names(xgb_importance_scores) <- xgb_importance$Feature
# Shapley values using the iml package
# Ensure the predictor and response data are correctly passed to the iml Predictor
predictor <- Predictor$new(rf_model, data = standardized_data[predictors], y = as.numeric(data$satisfaction))
# Calculate Shapley values for each predictor
shapley <- Shapley$new(predictor, x.interest = standardized_data[1, predictors])
shapley_values <- as.data.frame(shapley$results)
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
shapley_values_vector <- setNames(shapley_values_summary$phi, shapley_values_summary$feature)
# Ensure all components have matching lengths
print(length(predictors))                    # Should be the number of predictors
print(length(correlations))                  # Should match the number of predictors
print(length(standardized_coefs))            # Should match the number of predictors
print(length(usefulness))                    # Should match the number of predictors
print(length(mean_decrease_gini))            # Should match the number of predictors
print(length(shapley_values_vector))         # Should match the number of predictors
print(length(xgb_importance_scores))         # Should match the number of predictors
# Combine all results into a single data frame
results <- data.frame(
Variable = predictors,
Pearson_Correlation = correlations,
Standardized_Coefficients = standardized_coefs,
Usefulness = usefulness,
Mean_Decrease_Gini = mean_decrease_gini,
Shapley_Values = shapley_values_vector[predictors],
XGBoost_Importance = xgb_importance_scores[predictors]
)
# Install necessary packages if not already installed
required_packages <- c("tidyverse", "car", "randomForest", "iml", "data.table", "MASS", "relaimpo", "caret", "e1071", "xgboost")
new_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load necessary libraries
library(tidyverse)
library(car)
library(randomForest)
library(iml)
library(data.table)
library(MASS)
library(relaimpo)
library(caret)
library(knitr)
library(e1071)
library(xgboost)
# Load the data
data <- read.csv("/Users/danielgarciagomar/Documents/UCSD/MPP/Spring/Marketing Analytics/Quarto/quarto_website/projects/project4/data_for_drivers_analysis.csv")
data$satisfaction <- as.factor(data$satisfaction)  # Convert the response variable to a factor
# Verify the levels of the factor
print(levels(data$satisfaction))
# Display structure and first few rows to understand the data
str(data)
head(data)
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Train an XGBoost model
dtrain <- xgb.DMatrix(data = as.matrix(data[predictors]), label = as.numeric(data[[response]]))
xgb_model <- xgboost(data = dtrain, max.depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror", verbose = 0)
# Extract feature importance from XGBoost model
xgb_importance <- xgb.importance(model = xgb_model)
xgb_importance_scores <- xgb_importance$Gain
names(xgb_importance_scores) <- xgb_importance$Feature
# Shapley values using the iml package
# Ensure the predictor and response data are correctly passed to the iml Predictor
predictor <- Predictor$new(rf_model, data = standardized_data[predictors], y = as.numeric(data$satisfaction))
# Calculate Shapley values for each predictor
shapley <- Shapley$new(predictor, x.interest = standardized_data[1, predictors])
shapley_values <- as.data.frame(shapley$results)
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
shapley_values_vector <- setNames(shapley_values_summary$phi, shapley_values_summary$feature)
# Ensure all components have matching lengths
print(length(predictors))                    # Should be the number of predictors
print(length(correlations))                  # Should match the number of predictors
print(length(standardized_coefs))            # Should match the number of predictors
print(length(usefulness))                    # Should match the number of predictors
print(length(mean_decrease_gini))            # Should match the number of predictors
print(length(shapley_values_vector))         # Should match the number of predictors
print(length(xgb_importance_scores))         # Should match the number of predictors
# Combine all results into a single data frame
results3 <- data.frame(
Variable = predictors,
Pearson_Correlation = correlations,
Standardized_Coefficients = standardized_coefs,
Usefulness = usefulness,
Mean_Decrease_Gini = mean_decrease_gini,
Shapley_Values = shapley_values_vector[predictors],
XGBoost_Importance = xgb_importance_scores[predictors]
)
# Define the response variable and predictor variables
response <- "satisfaction"
predictors <- setdiff(names(data), c("brand", "id", response))
# Calculate Pearson correlations
correlations <- sapply(predictors, function(var) cor(data[[var]], as.numeric(data[[response]]), use = "complete.obs"))
correlations <- setNames(correlations, predictors)
# Standardize the predictor variables
standardized_data <- data
standardized_data[predictors] <- scale(data[predictors])
# Fit an ordinal logistic regression model
formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
olr_model <- polr(formula, data = data, Hess = TRUE)
# Extract standardized regression coefficients
standardized_coefs <- coef(summary(olr_model))[, "Value"]
standardized_coefs <- setNames(standardized_coefs, predictors)
# Calculate relative importance (usefulness) using relaimpo package
lm_model <- lm(as.formula(paste("as.numeric(satisfaction) ~", paste(predictors, collapse = " + "))), data = standardized_data)
rel_importance <- calc.relimp(lm_model, type = "lmg")
usefulness <- rel_importance$lmg
usefulness <- setNames(usefulness, predictors)
# Hyperparameter tuning for random forest model
control <- trainControl(method="cv", number=5)
tunegrid <- expand.grid(.mtry=c(2, 4, 6, 8, 10))
rf_tuned <- train(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data=data, method="rf", trControl=control, tuneGrid=tunegrid)
# Best model
best_rf_model <- rf_tuned$finalModel
# Fit the best random forest model
rf_model <- randomForest(as.formula(paste(response, "~", paste(predictors, collapse = " + "))),
data = data, mtry = best_rf_model$mtry, ntree = 500, importance = TRUE)
# Calculate the mean decrease in Gini coefficient
mean_decrease_gini <- importance(rf_model, type = 2)[,1]
mean_decrease_gini <- setNames(mean_decrease_gini, predictors)
# Train an XGBoost model
dtrain <- xgb.DMatrix(data = as.matrix(data[predictors]), label = as.numeric(data[[response]]))
xgb_model <- xgboost(data = dtrain, max.depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror", verbose = 0)
# Extract feature importance from XGBoost model
xgb_importance <- xgb.importance(model = xgb_model)
xgb_importance_scores <- xgb_importance$Gain
names(xgb_importance_scores) <- xgb_importance$Feature
# Shapley values using the iml package
# Ensure the predictor and response data are correctly passed to the iml Predictor
predictor <- Predictor$new(rf_model, data = standardized_data[predictors], y = as.numeric(data$satisfaction))
# Calculate Shapley values for each predictor
shapley <- Shapley$new(predictor, x.interest = standardized_data[1, predictors])
shapley_values <- as.data.frame(shapley$results)
shapley_values_summary <- shapley_values %>%
group_by(feature) %>%
summarize(phi = mean(phi), .groups = 'drop')
shapley_values_vector <- setNames(shapley_values_summary$phi, shapley_values_summary$feature)
# Ensure all components have matching lengths
print(length(predictors))                    # Should be the number of predictors
print(length(correlations))                  # Should match the number of predictors
print(length(standardized_coefs))            # Should match the number of predictors
print(length(usefulness))                    # Should match the number of predictors
print(length(mean_decrease_gini))            # Should match the number of predictors
print(length(shapley_values_vector))         # Should match the number of predictors
print(length(xgb_importance_scores))         # Should match the number of predictors
# Combine all results into a single data frame
results3 <- data.frame(
Variable = predictors,
Usefulness = usefulness,
Mean_Decrease_Gini = mean_decrease_gini,
Shapley_Values = shapley_values_vector[predictors],
XGBoost_Importance = xgb_importance_scores[predictors]
)
# Display the results in a table
kable(results3, caption = "Driver Analysis Results")
